{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "\n",
    "\n",
    "**FNNs** employ fixed-size vectors and have difficulties dealing with sequences of varying length: they fail to capture the temporal aspects of language.\n",
    "**RNNs** solve this problem with a cycle within its network connections:\n",
    "\n",
    "![image-3.png](img/RNN-unrolled.png)\n",
    "The activation value of the hidden layer depends on the current input as well as the activation value of the hidden layer from the previous time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "**Forward inference** is similar to what we saw with FNNs:\n",
    "$$\\begin{align}\n",
    "   &h_t = g(Uh_{t-1}+Wx_t)\\newline\n",
    "   &y_t=f(Vh_t)\n",
    "   \\end{align}$$\n",
    "This is an **incremental inference algorithm**, it can be interpreted as unrolling the network in time:\n",
    "![image.png](img/RNN_time.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Sentiment classification on a moview review dataset\n",
    "The following example is taken from: https://github.com/Harvard-IACS/2019-CS109B/blob/master/content/labs/lab6/cs109b-lab6-rnn-solutions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "<li>25000 movie reviews from IMDB, labeled either positive or negative</li>\n",
    "<li>In each review, words are indexed by their overall frequency</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sentence can be thought of as a sequence of words which have semantic connections across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, SimpleRNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import Flatten\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define some parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# take the 10000 most common words\n",
    "VOCABULARY_SIZE = 10000\n",
    "\n",
    "#Finite length of the review (avoid long sentences)\n",
    "MAX_REVIEW_LENGTH = 500\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, we would proceed by **tokenising** the text, but the IMDB dataset is already tokenised.\n",
    "Then **load the data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: [0 1]\n",
      "Number of unique words: 9998\n",
      "First review [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "Length of first and fifth review 218 147\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=VOCABULARY_SIZE)\n",
    "\n",
    "print(\"Categories:\", np.unique(y_train))\n",
    "print(\"Number of unique words:\", len(np.unique(np.hstack(X_train))))\n",
    "\n",
    "print('First review', X_train[0])\n",
    "print('Length of first and fifth review', len(X_train[0]) ,len(X_train[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now **pre-process data** by ensuring that all inputs have same sentence length and dimensions. We use Keras' `sequence.pad_sequences()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of first and fifth review after padding 500 500\n"
     ]
    }
   ],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=MAX_REVIEW_LENGTH)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=MAX_REVIEW_LENGTH)\n",
    "print('Length of first and fifth review after padding', len(X_train[0]) ,len(X_train[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FNNs with embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/FNN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build a single layer FNN with 250 nodes. Each input will be a 500-dim vector of tokens (remember padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 500, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 50000)             0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 250)               12500250  \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 13,500,501\n",
      "Trainable params: 13,500,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#instantiate the model. Sequential groups a linear stack of layers into the model.\n",
    "model_fnn = Sequential()\n",
    "\n",
    "#add the embedding layer\n",
    "model_fnn.add(Embedding(VOCABULARY_SIZE, EMBEDDING_DIM, input_length=MAX_REVIEW_LENGTH))\n",
    "\n",
    "#we flatten the input layer to one dimension\n",
    "model_fnn.add(Flatten())\n",
    "\n",
    "#hidden layer\n",
    "model_fnn.add(Dense(250, activation='relu'))\n",
    "\n",
    "#output layer \n",
    "model_fnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#configure model for training\n",
    "model_fnn.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "print(model_fnn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fit the model and evaluate its performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "196/196 - 15s - loss: 0.6931 - accuracy: 0.5051 - val_loss: 0.6929 - val_accuracy: 0.5116\n",
      "Epoch 2/2\n",
      "196/196 - 16s - loss: 0.6920 - accuracy: 0.5247 - val_loss: 0.6923 - val_accuracy: 0.5164\n",
      "Accuracy: 51.64%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_fnn.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "\n",
    "# Evaluation of the model\n",
    "scores = model_fnn.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar procedure, but here hidden states are created using the state variable from the previous timestep and the input at current time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 500, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,020,201\n",
      "Trainable params: 1,020,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#instantiate the model. Sequential groups a linear stack of layers into the model.\n",
    "model_rnn = Sequential()\n",
    "\n",
    "#add the embedding layer\n",
    "model_rnn.add(Embedding(VOCABULARY_SIZE, EMBEDDING_DIM, input_length=MAX_REVIEW_LENGTH))\n",
    "\n",
    "#add a fully-connected RNN with 100 units\n",
    "model_rnn.add(SimpleRNN(100))\n",
    "\n",
    "#output layer\n",
    "model_rnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "#configure model for training\n",
    "model_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_rnn.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "196/196 [==============================] - 29s 149ms/step - loss: 0.5553 - accuracy: 0.6896\n",
      "Epoch 2/2\n",
      "196/196 [==============================] - 28s 145ms/step - loss: 0.5162 - accuracy: 0.7413\n",
      "Accuracy: 68.13%\n"
     ]
    }
   ],
   "source": [
    "#fit the model\n",
    "model_rnn.fit(X_train, y_train, epochs=2, batch_size=128)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model_rnn.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During backpropagation, simple RNNs can have ***vanishing gradients***. In fact, calculating the derivative of the sigmoid:\n",
    "$$\\sigma(z)=\\frac{1}{1+e^{-x}}\\quad \\to \\quad \\frac{d\\sigma}{dz}=\\sigma(z)(1-\\sigma(z))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](img/LSTM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Long short-term memory or LSTMS handle the context in a more intuitive way:\n",
    "<ul>\n",
    "<li>Remove information no longer needed</li>\n",
    "<li>Add information likely to be needed lately</li>\n",
    "</ul>\n",
    "\n",
    "They introduce different gates: ***forget gate, add gate, output gate*** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 500, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,080,501\n",
      "Trainable params: 1,080,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "196/196 [==============================] - 76s 389ms/step - loss: 0.6930 - accuracy: 0.5096\n",
      "Epoch 2/2\n",
      "196/196 [==============================] - 79s 405ms/step - loss: 0.6930 - accuracy: 0.5106\n",
      "Accuracy: 51.45%\n"
     ]
    }
   ],
   "source": [
    "#instantiate the model. Sequential groups a linear stack of layers into the model.\n",
    "model_lstm = Sequential()\n",
    "\n",
    "#add the embedding layer\n",
    "model_lstm.add(Embedding(VOCABULARY_SIZE, EMBEDDING_DIM, input_length=MAX_REVIEW_LENGTH))\n",
    "\n",
    "#add LSTM layer with output dimensionality 100\n",
    "model_lstm.add(LSTM(100))\n",
    "\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_lstm.compile(loss='binary_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "print(model_lstm.summary())\n",
    "\n",
    "model_lstm.fit(X_train, y_train, epochs=2, batch_size=128)\n",
    "\n",
    "# Final evaluation of the model\n",
    "scores = model_lstm.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note number of parameters compared to simple RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to improve the accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More advanced model (taken from https://www.tensorflow.org/tutorials/text/text_classification_rnn#create_the_model):\n",
    "\n",
    "![image.png](img/tensorflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([Embedding(input_dim=VOCABULARY_SIZE,\n",
    "                              output_dim=64,\n",
    "                              # Use masking to handle the variable sequence lengths\n",
    "                              mask_zero=True),\n",
    "                    Bidirectional(LSTM(64)),\n",
    "                    Dense(64, activation='relu'),Dense(1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra stuff (better not to show it..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(review_index):\n",
    "    index = imdb.get_word_index()\n",
    "    reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
    "    decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in X_train[review_index]] )\n",
    "    print(decoded) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # here it is the first ever episode of friends where we get introduced to control freak monica # # cox newly divorced ross # david # # # # lisa # unknown actor and ladies man matt le # and very sarcastic chandler bing matthew perry this is how the scene starts off until we introduced to the # and final friend # kid rachel green jennifer aniston br br the episode is better than most people give credit for like any new sitcom the first episode isn't always fantastic the acting in this episode isn't great because the cast cannot identify and # really believable in their new characters apart from # and perry who shine br br matt le # man his acting was down right dreadful because until later he gets more confident but i think he tries to be funny but at most fails br br david # why does he over # every word he cannot speak normally but he became one of the funniest characters in later seasons but he isn't confident and i cannot # with him jennifer aniston looks hot and does a good job as rachel green but we only see the real rachel later in the 1st season # cox looks quite # in this episode its worrying she looks totally different now more # she acting is a little # but # is in this 20 minute pilot lisa # and matthew perry i'm doing these two together because their comic timing and acting quality was superb and for lisa this was one of her first roles and she is so natural as # # and matthew perry is just matthew perry playing himself basically the episode quality does improve later such as the sets they looks dark and creepy in this episode and makes them seem # the acting is ok the characters gain confidence with each new scene and i am proud this is the pilot i hope we see the friends reunite cause they will always be there for us\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "have any of these characters actually doing something exciting for once neil is a geek who runs his own very small video rental shop he and his other geek friends usually hang out around the shop and watching movies together while # about them afterward but # life is completely turned around when # walks into his store she's an eccentric woman who hides a little secret from him anyway after some dates they decide to see each other the problem is # is a person who keep doing prank jokes on neil and can't really doing something normal whereas neil is completely opposite to hers the question is is they are going to be in love at the end you bet br br watching the detectives is a clich√© romantic comedy to its core and they made it even worst by pretending to be something else from the first couple of set up we know that neil is pretty laid back guy who didn't really commit to anything and then # enters the scene looking all weird and annoying so at this point we all know that we're going to sit though all meaningless situations to find out how they're going to end up in the end is it worth waiting for i would say no br br as i said they tried to give something more for the audience watching the detectives is trying to talk about commitment to observe how far people go to reach for something they desire we knew in the end that insane things that # has done is all the test how far neil is ready to go to win her heart or whatever well i think it is completely # this movie will end pretty quick if neil just said to himself forget about it that girl is one of a # after # with many great directors recently danny # sunshine and 28 days later ken # the wind that shakes the # and neil # breakfast on # to name a few it's pretty weird choice for cillian murphy to make a movie with one of broken lizard comedy # paul # by all means he's not bad as usual but such a talent actor like him shouldn't be wasting his time in the movie like this on the other hand lucy liu is dreadfully awful as # her acting is a mess i mean it's all over the place and so over the top tony montana would have been proud br br the last but not least mistake that movie made is a completely irrelevant title you simply can't really connect a # between the plot and its title and then you will end up thinking that it makes no sense at all in short watching the detectives is pleasant if forgettable motion picture that you might have a chance to catch it on cable tv so quick that you couldn't imagine br br # rating 1 5 4\n"
     ]
    }
   ],
   "source": [
    "#Good review\n",
    "decode(449)\n",
    "\n",
    "print('\\n' * 3)\n",
    "\n",
    "#Bad review\n",
    "decode(7337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_bad = X_train[7737]\n",
    "text_good = X_train[449]\n",
    "texts = (text_bad, text_good)\n",
    "padded_texts = sequence.pad_sequences(texts, maxlen=MAX_REVIEW_LENGTH, value = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.48101017]\n",
      " [0.49473682]] \n",
      " [[0.5246494 ]\n",
      " [0.46627226]] \n",
      " [[0.49783504]\n",
      " [0.5042565 ]]\n"
     ]
    }
   ],
   "source": [
    "predictions_fnn = model_fnn.predict(padded_texts)\n",
    "predictions_rnn = model_rnn.predict(padded_texts)\n",
    "predictions_lstm = model_lstm.predict(padded_texts)\n",
    "print(f'{predictions_fnn} \\n {predictions_rnn} \\n {predictions_lstm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
